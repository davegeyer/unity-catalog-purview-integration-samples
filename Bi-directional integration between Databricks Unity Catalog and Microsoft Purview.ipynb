{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2b11cc-7c5e-48fb-894f-0f9dec2020a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Bi-directional integration between Databricks Unity Catalog and Microsoft Purview**\n",
    "### Discoverability and interoperability samples notebook\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/1*3Kw_fP46Wu64rwOkCWVMcA.png)\n",
    "\n",
    "__Changes:__\n",
    "\n",
    "* [November 05, 2024] Initial release\n",
    "\n",
    "__Requirements:__\n",
    "\n",
    "* DBR 13.3+\n",
    "* Python 3.7+\n",
    "\n",
    "__Resources:__\n",
    "\n",
    "* [PyApacheAtlas](https://github.com/wjohnson/pyapacheatlas) | [Documentation](https://wjohnson.github.io/pyapacheatlas-docs/latest/)\n",
    "* [Azure Purview Data Map SDK for Python](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-purview-catalog/1.0.0b4/index.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Author:_ Dave Geyer | <dave.geyer@databricks.com>     \n",
    "_Last Modified:_ November 05, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the following blog post on Medium: [Bi-directional integration between Databricks Unity Catalog and Microsoft Purview](https://medium.com/p/79452911f2f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6168bfea-596d-45de-a5c8-301059337505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 0. Introduction\n",
    "Azure Databricks Unity Catalog and Microsoft Purview together offer a comprehensive framework for data cataloging and governance. Through programmatic interaction with these platforms, we can go beyond basic discoverability to perform tasks like bulk loading and creating custom lineage.\n",
    "\n",
    "This notebook provides a guide on how to use Microsoft Purview with Azure Databricks Unity Catalog from within an Azure Databricks Notebook. This guide covers various aspects such as:\n",
    "* Initial setup and authentication for Azure Databricks and Microsoft Purview.\n",
    "* Utilizing PyApacheAtlas and the Microsoft Purview Data Map SDK for data asset exploration and searching.\n",
    "* Synchronizing data classifications between Microsoft Purview and Azure Databricks Unity Catalog\n",
    "* Enhancing Microsoft Purview with AI-generated table and column descriptions from Azure Databricks Unity Catalog.\n",
    "\n",
    "\n",
    "From the [Microsoft Purview documentation](https://medium.com/r/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fpurview%2Ftutorial-custom-types), an asset is a metadata element that describes a digital or physical resource. Microsoft Purview's flexible type system, based on [Apache Atlas](https://medium.com/r/?url=https%3A%2F%2Fatlas.apache.org%2F2.0.0%2FTypeSystem.html), allows for an expansive definition of assets, including databases, files, business policies, and more. By recognizing the properties and inheritance of these types, users can customize and extend these definitions to suit emerging business needs.\n",
    "\n",
    "All metadata objects (assets) managed by Microsoft Purview are modeled using type definitions. Understanding the Type System is fundamental to creating new custom types in Microsoft Purview.\n",
    "\n",
    "Essentially, a Type can be seen as a Class from Object Oriented Programming (OOP):\n",
    "* It defines the properties that represent that type.\n",
    "* Each type is uniquely identified by its name.\n",
    "* A type can inherit from a superType. This is an equivalent concept as inheritance from OOP. A type that extends a superType will inherit the attributes of the superType.\n",
    "\n",
    "Apache Atlas has a few pre-defined system types that are commonly used as superTypes. For example:\n",
    "* **Referenceable**: This type represents all entities that can be searched for using a unique attribute called qualifiedName.\n",
    "* **Asset**: This type extends from Referenceable and has other attributes such as name, description, and owner.\n",
    "* **DataSet**: This type extends Referenceable and Asset. Conceptually, it can be used to represent a type that stores data. Types that extend DataSet can be expected to have a Schema. For example, a SQL table.\n",
    "* **Lineage**: Lineage information helps one understand the origin of data and the transformations it may have gone through before arriving in a file or table. Lineage is calculated through DataSet and Process: DataSets (input of process) impact some other DataSets (output of process) through Process.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/0*UHC5GX4ZRUQ5o5jT.png)\n",
    "\n",
    "In this notebook, we will focus on the DataSet superType and will not cover asset relationships or how to define custom entity types, such as ML models or Databricks Volumes. I plan to cover that in a future post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811c39de-41a8-4d74-827e-261053869684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Getting started\n",
    "To start, we must initialize the necessary libraries and authenticate with Azure to ensure that our environment is ready for the following steps. All code is run within an Azure Databricks Notebook.\n",
    "\n",
    "Please note, you must use DBR 13.3+, and Python 3.7+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7d9e6f-b34b-4207-ae85-7139c975bb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1a. Initialize libraries\n",
    "We will need the following libraries:\n",
    "1. [_azure-identity_](https://pypi.org/project/azure-identity/) - The Azure Identity library provides Microsoft Entra ID (formerly Azure Active Directory) token authentication support across the Azure SDK.\n",
    "2. [_pyapacheatlas_](https://pypi.org/project/pyapacheatlas/) - PyApacheAtlas lets you work with the Azure Purview and Apache Atlas APIs in a Pythonic way. Supporting bulk loading, custom lineage, custom type definition and more from an SDK and Excel templates / integration.\n",
    "3. [_azure-purview-account_](https://pypi.org/project/azure-purview-account/) - Azure Purview Account client library for Python\n",
    "4. [_azure-purview-datamap_](https://pypi.org/project/azure-purview-datamap/) - Microsoft Purview Data Map provides the foundation for data discovery and data governance. Microsoft Purview Data Map is a cloud native PaaS service that captures metadata about enterprise data present in analytics and operation systems on-premises and cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240a2a05-0c5a-4517-8a5b-9d8a6d78bf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-identity pyapacheatlas azure-purview-account azure-purview-datamap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d532705-6bf4-498a-83fd-c1aa35338969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1b. Authentication\n",
    "\n",
    "Next, we will define the credentials used to authenticate into Azure. PyApacheAtlas and the Purview Data Map SDK have different authentication functions, resulting in the creation of two separate credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8ff256-3f52-42e8-8cfe-34c129e26454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "def authenticate(tenant_id, client_id, client_secret):\n",
    "    atlas_credential = ServicePrincipalAuthentication(tenant_id, client_id, client_secret)\n",
    "    datamap_credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
    "    return atlas_credential, datamap_credential\n",
    "\n",
    "credentials = authenticate(\n",
    "    dbutils.secrets.get(\"scope_name\", \"tenant_id\"),\n",
    "    dbutils.secrets.get(\"scope_name\", \"client_id\"),\n",
    "    dbutils.secrets.get(\"scope_name\", \"client_secret\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about secret management in the Azure Databricks documentation: [Secret management](https://medium.com/r/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fdatabricks%2Fsecurity%2Fsecrets%2F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1804ee0-22ae-4b53-8bf6-7d30a4c0de4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1c. Create clients\n",
    "\n",
    "Next, we will create three separate clients to build our requests and send these requests to Purview using the `send_request` method.\n",
    "\n",
    "We will create three clients:\n",
    "1. [Purview Client](https://wjohnson.github.io/pyapacheatlas-docs/latest/core/client/purviewclient.html) - PyApacheAtlas - Explore type definitions\n",
    "2. [Purview Discovery Client](https://wjohnson.github.io/pyapacheatlas-docs/latest/core/api/pyapacheatlas.core.discovery.purview.PurviewDiscoveryClient.html#pyapacheatlas.core.discovery.purview.PurviewDiscoveryClient) - PyApacheAtlas - Search and browse assets\n",
    "3. [Purview Datamap Client](https://learn.microsoft.com/en-us/rest/api/purview/datamapdataplane/operation-groups?view=rest-purview-datamapdataplane-2023-09-01) - Azure Purview Datamap SDK - Search and browse assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60495f7f-9fc9-410e-a47a-23299cfe66ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyapacheatlas.core import PurviewClient\n",
    "from pyapacheatlas.core.discovery import PurviewDiscoveryClient\n",
    "from azure.purview.datamap import DataMapClient\n",
    "\n",
    "account_name = \"purviewUC\"\n",
    "\n",
    "atlas_client = PurviewClient(account_name, authentication=credentials[0])\n",
    "discovery_client = PurviewDiscoveryClient(f\"https://{account_name}.purview.azure.com/catalog/api\", authentication=credentials[0])\n",
    "datamap_client = DataMapClient(f\"https://{account_name}.purview.azure.com\", credential=credentials[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0ad9fd-329b-4e77-a816-8c47630dc84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Exploring and searching data assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ac03f6-81de-476b-adb7-9493c18e89c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2a. PyApacheAtlas type definition retreival\n",
    "Let's start by reviewing the definitions for the different entity types in Microsoft Purview. This understanding is important for efficiently using the APIs available to us.\n",
    "\n",
    "First, we will get all type definitions from Purview by calling the PyApacheAtlas client's `get_all_typedefs()` method and flatten the results into a single-level dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed5d99a-099d-4401-92ff-1ce24f022069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_type_definitions(client):\n",
    "    all_types = atlas_client.get_all_typedefs()\n",
    "    return [{key: value for key, value in flatten_dict(type_def).items()} for type_def in all_types.get(\"entityDefs\", [])]\n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "get_type_definitions(atlas_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema for entity types is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- attributeDefs: array (nullable = true)\n",
    " |    |-- element: map (containsNull = true)\n",
    " |    |    |-- key: string\n",
    " |    |    |-- value: string (valueContainsNull = true)\n",
    " |-- businessAttributeDefs.PurviewDataQuality: array (nullable = true)\n",
    " |    |-- element: map (containsNull = true)\n",
    " |    |    |-- key: string\n",
    " |    |    |-- value: string (valueContainsNull = true)\n",
    " |-- category: string (nullable = true)\n",
    " |-- createTime: long (nullable = true)\n",
    " |-- createdBy: string (nullable = true)\n",
    " |-- description: string (nullable = true)\n",
    " |-- guid: string (nullable = true)\n",
    " |-- lastModifiedTS: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- options.purviewEntityExtDef: string (nullable = true)\n",
    " |-- options.schemaAttributes: string (nullable = true)\n",
    " |-- relationshipAttributeDefs: array (nullable = true)\n",
    " |    |-- element: map (containsNull = true)\n",
    " |    |    |-- key: string\n",
    " |    |    |-- value: string (valueContainsNull = true)\n",
    " |-- serviceType: string (nullable = true)\n",
    " |-- subTypes: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- superTypes: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- typeVersion: string (nullable = true)\n",
    " |-- updateTime: long (nullable = true)\n",
    " |-- updatedBy: string (nullable = true)\n",
    " |-- version: long (nullable = true)\n",
    " |-- options.defaultRenderedLineage: string (nullable = true)\n",
    " |-- options.schemaElementsAttribute: string (nullable = true)\n",
    " |-- options.dataTypeAttribute: string (nullable = true)\n",
    " |-- options.derivedLineageSources: string (nullable = true)\n",
    " |-- options.searchNameAttribute: string (nullable = true)\n",
    " |-- options.schemaElementsAttributesList: string (nullable = true)\n",
    " |-- options.displayTextAttribute: string (nullable = true)\n",
    " |-- businessAttributeDefs.test: array (nullable = true)\n",
    " |    |-- element: map (containsNull = true)\n",
    " |    |    |-- key: string\n",
    " |    |    |-- value: string (valueContainsNull = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55903c3f-e848-44cf-9d5d-808e978b8156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using the type definition schema above, let's look at some of the entity properties:\n",
    "\n",
    "* **Category** field describes in what category your type is. The list of categories supported by Apache Atlas can be found here.\n",
    "* **ServiceType** field is useful when browsing assets by source type in Microsoft Purview. The service type will be an entry point to find all assets that belong to the same service type - as defined on their type definition.\n",
    "* **SuperTypes** describes the \"parent\" types you want to \"inherit\" from.\n",
    "* **schemaElementsAttributes** from options influences what appears in the Schema tab of your asset in Microsoft Purview.\n",
    "* **relationshipAttributeDefs** are calculated through the relationship type definitions. In our JSON, we can see that schemaElementsAttributes points to the relationship attribute called columns - which is one of elements from relationshipAttributeDefs array, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark_df.select(\"category\",\"serviceType\",\"superTypes\",\"`options.schemaElementsAttribute`\",\"relationshipAttributeDefs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://cdn-images-1.medium.com/max/1000/0*Gu43zWP5WfcNc9QT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de88bb6-09c6-465a-9c5b-26c876427899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2b. Retrieve Type Definitions with Purview Data Map SDK\n",
    "Here's how to replicate the above process using the Microsoft Purview Data Map SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31b723f-c8e4-4e73-8e29-462152790146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_type_defs = datamap_client.type_definition.get().as_dict()\n",
    "entity_defs = all_type_defs.get(\"enumDefs\", [])\n",
    "flattened_entity_defs = []\n",
    "\n",
    "for entity_def in entity_defs:\n",
    "    flattened_def = flatten_dict(entity_def)\n",
    "    flattened_entity_defs.append(flattened_def)\n",
    "\n",
    "table_df = pd.DataFrame(flattened_entity_defs)\n",
    "spark_df = spark.createDataFrame(flattened_entity_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you can retrieve entity type definitions using both PyApacheAtlas and the Purview Data Map SDK, the Purview Data Map SDK is simpler to use, and better suited for building custom integrations with Purview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e562aafb-bcf7-4215-b1c0-f24348b4466f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. PyApacheAtlas Discovery\n",
    "Now that we have a better understanding of Purview's type system, we will use the PyApacheAtlas Search API to search existing assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fdaed5c-e430-48a7-9426-583d1dcf0644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3a. Search for data assets with PyApacheAtlas\n",
    "Let's define a search term, this time, \"stats\", which will look for any entity with the term \"stats\" in it. We'll flatten the response and format the output into a single, user-friendly dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31a55f6-d952-4775-bb4a-d0690a3e7c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_search_results(search_results):\n",
    "    entity_defs = list(search_results)\n",
    "    \n",
    "    flattened_search_results = []\n",
    "\n",
    "    for entity_def in entity_defs:\n",
    "        flattened_def = flatten_dict(entity_def)\n",
    "        flattened_search_results.append(flattened_def)\n",
    "    \n",
    "    spark_df = spark.createDataFrame(flattened_search_results)\n",
    "    \n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40bd952-129e-40a9-a991-65b0b853980d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_term = \"stats\"\n",
    "search_results = discovery_client.search_entities(query=search_term)\n",
    "processed_results = process_search_results(search_results)\n",
    "display(processed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following output, in which we see the entity's description, name, and entity type, among other things:\n",
    "\n",
    "![image.png](https://cdn-images-1.medium.com/max/1000/0*sBnWAGvAkxJXUFPy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c2bd82-7049-4f35-a36a-07bee8c1b643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3b. Browse Data Assets with PyApacheAtlas\n",
    "Similar to the search functionality, we can also browse entities across different facets within Microsoft Purview. In this case, let's look at all entities that are classified as a databricks_view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf2fd76-dd91-4306-b2ce-9bf4abff9d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_browse_results(browse_results):\n",
    "    entities = browse_results\n",
    "    flattened_data = [flatten_dict(entity) for entity in entities]\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityType = 'databricks_view'\n",
    "browse_results = discovery_client.browse(entityType=entityType, api_version='2022-03-01-preview')\n",
    "browse_results_filtered = browse_results[\"value\"]\n",
    "processed_results = process_browse_results(browse_results_filtered)\n",
    "display(processed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following output, showing all of the different registered views:\n",
    "\n",
    "![image.png](https://cdn-images-1.medium.com/max/1000/0*IGMNl5wl_UJVnQel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e6cd5a-57c9-4d3a-803a-6d756ef39690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Discovery with Purview Data Map Client SDK\n",
    "Using the Purview Data Map Client SDK, we can use a single \"query\" method to search for/browse our assets (instead of the two separate queries that we used with PyApacheAtlas). This time, let's look for entities with the keyword \"orders,\" although you can define a search body that combines multiple search parameters into a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b800a7-fc6a-42b2-86f1-0c0e8f9aa4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list) or isinstance(v, tuple):\n",
    "            for i, item in enumerate(v):\n",
    "                new_item_key = f\"{new_key}{sep}{i}\" if new_key else str(i)\n",
    "                items.extend(flatten_dict({i: item}, new_item_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d537b935-83f8-4a03-b0b0-818bc5cb5d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"keywords\": \"orders\",\n",
    "  \"limit\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68444fd0-fe4a-4472-8863-89b128b74f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_results = datamap_client.discovery.query(body)\n",
    "search_results_values = list(search_results.value)\n",
    "\n",
    "flattened_search_results = [flatten_dict(entity) for entity in search_results_values]\n",
    "flattened_df = spark.createDataFrame(flattened_search_results)\n",
    "display(flattened_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following output:\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/1*XchqUceMtDt64UMiTLO1RA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611c1b9f-8649-4b8c-854d-980dc59337d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see from the results that we have multiple tables with the same name - from a discoverability standpoint, how can we differentiate whether this is a function of poor data quality or due to different teams needing similar table names? We have a couple of approaches.\n",
    "\n",
    "First, the results have unique identifiers that we can use to distinguish between entities. Additionally, not shown above, the entities belong to different collections and are represented by a unique identifier.\n",
    "\n",
    "We can modify our query to search for entities \"orders\", and also that belong to a specific `collectionId`. The `collectionId` can be extracted from the URL by navigating to the collection in your browser. The URL has the format of: `https://purview.microsoft.com/datamap/governance/main/datasource/domains?tid=<domainID>&collection=<collectionId>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5f33ce-1b75-47c3-a454-00738eeb8d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"keywords\": \"orders\",\n",
    "  \"limit\": 1000,\n",
    "  \"filter\": {\n",
    "    \"collectionId\": \"nqddbh\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Let's take a look at the output:\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/1*ZkFl9CDrZJY2qJnLohpHaw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b43c6a5-a1f6-4893-b57b-dc5ec84faacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Synchronizing Data Classifications and Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a. Pull Purview Data Classifications\n",
    "Improving discoverability in Purview by adding additional business attributes, such as classifications for different entities, can significantly enhance metadata quality in Unity Catalog. This enhancement not only makes data easier to find but also improves the semantic engine within Databricks. These advancements result in increased productivity for your team and strengthen your governance practices.\n",
    "\n",
    "Let's begin by examining the table of interest in its current state within the Unity Catalog Explorer UI. We can see that the table currently has no associated tags.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/0*f1GrbujaCdxsVA-T.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03c2597-08cf-4bee-ace0-145eb8f94dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's start by extracting out all entities where we have a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2831af-a130-4bef-b4cd-fb0be4ee7ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "update_classification = flattened_df.where(\"`classification.0.0` IS NOT NULL\")\n",
    "result = datamap_client.entity.get(guid='guid').entity\n",
    "flattened_result = flatten_dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3883c24a-bfb8-47a8-a102-482face1e9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, let's define a User-Defined Function (UDF) that will allow us to extract the entity attributes from Purview, and apply this to many entities at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80a7afa-e389-44ff-a3f7-1358cac333f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, MapType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=MapType(StringType(), StringType()))\n",
    "def fetch_and_flatten(guid):\n",
    "    result = datamapClient.entity.get(guid=guid).entity\n",
    "    flattened_result = flatten_dict(result)\n",
    "    return flattened_result\n",
    "\n",
    "flattened_df = update_classification.withColumn(\"flattened_data\", fetch_and_flatten(update_classification.id))\n",
    "\n",
    "merged_keys = flattened_df.select(\"flattened_data\").first()[0].keys()\n",
    "\n",
    "schema = StructType([\n",
    "StructField(key, StringType(), True) for key in merged_keys\n",
    "])\n",
    "\n",
    "final_df = flattened_df.rdd.map(lambda x: tuple(x.flattened_data.values())).toDF(schema)\n",
    "\n",
    "combined_df = update_classification.join(final_df, update_classification.id == final_df.guid)\n",
    "\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a column that concatenates the table catalog, schema, and table name in order to define the fully qualified name, which will help us apply multiple tags at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "filtered_df = combined_df.select(\"`attributes.catalogName`\", \"`attributes.schemaName`\", \"name\", \"`classification.0.0`\", \"`attributes.owner`\", \"`attributes.tableType`\")\n",
    "\n",
    "# Concatenate attributes.catalogName, attributes.schemaName, and name with \".\" character\n",
    "concatenated_col = concat(col(\"`attributes.catalogName`\"), lit(\".\"), col(\"`attributes.schemaName`\"), lit(\".\"), col(\"name\"))\n",
    "filtered_df_with_concat = filtered_df.withColumn(\"fq_name\", concatenated_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b4ee0f-704f-4c2f-b25f-62852cf870fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Iterate through and apply tags to tables or views within UC using the classification of the entity from Purview. This can also be extended to column-level comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee95b0f-6d4b-4def-a7e0-d14a4af3a0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in filtered_df_with_concat.collect():\n",
    "    fq_name = row[\"fq_name\"]\n",
    "    classification_value = row[\"classification.0.0\"].split(\".\")[-1]\n",
    "    obj_type = row[\"attributes.tableType\"]\n",
    "\n",
    "    if obj_type == \"MANAGED\":\n",
    "        spark.sql(f\"ALTER TABLE {fq_name} SET TAGS ('Purview' = '{classification_value}')\")\n",
    "    else:\n",
    "        print(f\"Skipping {fq_name} because it's not a table or a view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba9e31a-c686-4c91-8219-1d348b3add62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After the command finishes, go back and review the table in the Catalog Explorer UI. We can see that the tags were automatically added to the table.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1000/0*y9zJLh1tUy60AunH.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f8d047-abb0-4480-815a-f74428dec992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Update Microsoft Purview Unity Catalog with AI-Generated Comments\n",
    "\n",
    "Adding comments and documentation to your enterprise data is a thankless task but when your organization's tables are sparsely documented, both humans and AI struggle to find the right data for accurately answering your data questions. [AI-generated comments](https://medium.com/r/?url=https%3A%2F%2Fdocs.databricks.com%2Fen%2Fcatalog-explorer%2Fai-comments.html) address this by automating the manual process of adding descriptions to tables and columns through the magic of generative AI.\n",
    "\n",
    "We recently made significant improvements to the underlying algorithms supporting [AI-generated comments](https://medium.com/r/?url=https%3A%2F%2Fdocs.databricks.com%2Fen%2Fcatalog-explorer%2Fai-comments.html) in Unity Catalog. Learn more about this update: [How we improved DatabricksIQ LLM quality for AI-generated table comments](https://medium.com/r/?url=https%3A%2F%2Fwww.databricks.com%2Fblog%2Fhow-we-improved-databricksiq-llm-quality-ai-generated-table-comments).\n",
    "\n",
    "Currently, the integration between Unity Catalog and Purview is a \"pull\" mechanism, in which Purview must scan Unity Catalog for any changes. However, Microsoft Purview does not currently support [incremental data scans](https://medium.com/r/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fpurview%2Fregister-scan-azure-databricks-unity-catalog) from Unity Catalog - only full scans.\n",
    "\n",
    "We can take advantage of the Microsoft Purview APIs to \"push\" ad-hoc changes back to Purview without doing a full data scan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31e16451-5881-482b-a59e-3507cecfa740",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6a. Update table comments\n",
    "We can also see that the current state of the table in Purview has no table description. Let's change this! We will use Unity Catalog's AI-generated table comment to update the Purview asset. Accept the recommendation in the UI, and then, let's update the entity in Purview to resemble these changes. In the future, this should be something that can be done via API.\n",
    "\n",
    "Unity Catalog             |  Purview\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://imgur.com/ekFI081.png)  |  ![](https://imgur.com/ePv4IxZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4da682-8bcd-464d-b052-e93783d7d7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You should start by fetching the table comment from Unity Catalog and passing it to the Purview API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f4a1f2-68b8-4efa-bb23-c62f3bdcb36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "table_def=spark.sql(\"DESCRIBE EXTENDED davegeyer.iot_platform.turbine_training_dataset_ml\")\n",
    "table_comment = table_def.filter(col(\"col_name\") == \"Comment\").select(\"data_type\").first()[0]\n",
    "table_comment = str(table_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97bfc2c-9381-4ab5-b241-3e340ba0bc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "fqn = \"databricks://<entity-guid>/catalogs/<catalog-name>/schemas/<schema-name>/tables/<table-name>\"\n",
    "\n",
    "entity_input = {\n",
    "  \"referredEntities\": {},\n",
    "  \"entity\": {\n",
    "    \"typeName\": \"databricks_table\",\n",
    "    \"attributes\": {\n",
    "        'qualifiedName': f\"{fqn}\",\n",
    "        'name': '<table-name>',\n",
    "        \"userDescription\": table_comment,\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = datamapClient.entity.create_or_update(body=entity_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c9f26d3-79ea-4de1-a660-c2914a890394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's check Purview again. The asset description now matches the description in Unity Catalog!\n",
    "\n",
    "![Table comment updated](https://imgur.com/V7AqGZ4.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0186f816-02aa-4096-b415-007fddcdb1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6b. Update column comments\n",
    "We can take the same approach to update the column descriptions of a table within Purview. Let's use the AI-generated column comments in Unity Catalog, and push these updates to Purview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55bbc2b9-810b-408f-9f0e-9ea7d9b43ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unity Catalog             |  Purview\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://imgur.com/9WA5njE.png)  |  ![](https://imgur.com/FOWSS8J.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's extract all the column comments using `system.information_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df8dfaf-21c9-4afe-879d-dab81519c6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_catalog = 'davegeyer'\n",
    "table_schema = 'iot_platform'\n",
    "table_name = 'turbine_training_dataset_ml'\n",
    "\n",
    "columnComments = spark.sql(f\"SELECT * FROM system.information_schema.columns WHERE table_catalog = '{table_catalog}' AND table_schema = '{table_schema}' AND table_name = '{table_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e7f7a0-fa04-411b-b823-3c76da4988fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fqtable_name = \"databricks://<entity-id>/catalogs/davegeyer/schemas/iot_platform/tables/turbine_training_dataset_ml\"\n",
    "columnComments = columnComments.withColumn(\"column_qn\", concat(lit(fqtable_name + \"/columns/\"), col(\"column_name\")))\n",
    "columnCommentsDict = columnComments.select(\"column_qn\", \"comment\", \"column_name\").toPandas().set_index(\"column_qn\").to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6429dd-2786-490f-98bb-0ea6e3bb44f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for column_qn, values in columnCommentsDict.items():\n",
    "    body = {\n",
    "        \"referredEntities\": {},\n",
    "        \"entity\": {\n",
    "            \"typeName\": \"databricks_table_column\",\n",
    "            \"attributes\": {\n",
    "                \"qualifiedName\": column_qn,\n",
    "                \"name\": values[\"column_name\"], \n",
    "                \"userDescription\": values[\"comment\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = datamapClient.entity.create_or_update(body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh Purview, and take another look at the column descriptions. Woohoo! It's updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae8b149-2594-4512-af3b-30ed8d6db334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Column comments updated](https://imgur.com/dN6KT9a.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfb5a4b-562d-47a0-8bb2-fb7f4e0e16b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Conclusion\n",
    "In this notebook, we saw how to easily search for data assets stored within Microsoft Purview, synchronize classifications with Azure Databricks Unity Catalog, and push incremental changes back to Purview to enhance and maintain the most up-to-date metadata for your data assets.\n",
    "The integration of Purview and Unity Catalog is getting better (Lineage was added in October 2024), but there are still times when using these APIs is the way to go. For example: \n",
    "* **Existing Purview Users:** If your organization already uses Purview and you want to add Databricks as a data source, you may want to use the APIs to incorporate into your data stewardship and metadata curation process.\n",
    "* **Syncing Tags to Unity Catalog:** For those with existing tags and/or business glossaries in Purview, using APIs to sync tags to Unity Catalog is essential. These tag integration benefits include data discoverability, cost management and attribution, and governance.\n",
    "\n",
    "Have any comments or suggestions? Please reach out and let me know.\n",
    "\n",
    "Note: The views/opinions expressed in the blog are my own and do not necessarily represent the views/opinions of Databricks.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1098367650413546,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Azure Databricks Unity Catalog and Microsoft Purview - Discoverability integration samples",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
